---
layout: post
title: python 核心编程
category: python
tags: Re
keywords: Python 核心编程 正则表达式 TCP  
description: Re
---

>高级主题读书笔记

## 15 正则表达式

"|" 匹配多个正则表达式模式
"." 匹配任意一个单个的字符(换行符除外)
"^" 或者 "\A"  匹配开头
"$" 或者 "\Z"  匹配结尾
"\b" 匹配单词边界；为了跟退格键'\b'区分开匹配用"//b"或者r'\b'表示。 
"\B" 匹配单词中间部分的边界。例如：\Bthe   匹配包含"the"但不以the开头的单词
"[]" [A-Z] a-z  0-9   :匹配括号中任意一个字符
 [^] 括号内表示否定 
 闭包操作符:
*,  匹配左边的正则表达式0次或0次以上
+,  匹配左边的正则表达式1次或1次以上
?,  匹配左边的正则表达式0次或1次
{N}，{N,M}  匹配左边的正则表达式N次, N-M次
0?[1-9]  ：1月到9月数字的表示形式 一位或2位
[dn]ot?  : 字符d或0，后面加个"o" 最后是最多一个字符"t"
特殊字符表示字符集：
\d 十进制数字; 表示不匹配：\D
\w 整个字符数字的字符集:"A-Za-z0-9_";  \W
\s 空白字符  ; \S
用圆括号()组建组

### 15.1 Re模块：核心函数和方法
compile()
match()   从字符串开头进行匹配
search()  在字符串中查找一个模式
findall()
finditer()
split()
sub() 匹配的部分进行替换，返回替换后的字符串
subn() 匹配替换，还返回一个表示替换次数的数字，返回一个元组
group()
groups()
例如：
>>   m = re.match('(\w\w\w)-(\d\d\d)', 'abc-123')
>>	 m.group()
  'abc-123'
>>   m.group(1)
  'abc'
>>   m.groups()
  ('abc', '123')

### 15.4

from random import randint, choice   #用from import 只导入一个包其中几个函数 

正则表达式本身默认是贪心匹配的：会尽量抓取满足匹配的最长字符串
用非贪婪操作符"?" ：要求正则表达式匹配的字符越少越好

## 16 网络编程

套接字家族有两种类型：
1基于文件型的：AF_UNIX (AF_LOCAL）
2基于网络型的: AF_INET, AF_NETLINK

套接字中分两种类型：
面向连接(虚电路或流套接字)：SOCK_STREAM, 传输控制协议(TCP)，IP查找网络主机,这样形成整个系统即 TCP/IP
无连接：SOCK_DGRAM, 用户数据报协议(UDP), UDP/IP

		import socket
		tcpSock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

		from socket import *
		tcpSock = socket(AF_INET, SOCK_STREAM)

服务器套接字函数：
s.bind()
s.listen()
s.accept()
客户端套接字函数：
s.connect()
s.connect_ex()    出错时返回错误码而不是抛异常
公共用途的套接字函数：
s.recv()    TCP
s.send()
s.sendall() 完整发送TCP数据
s.recvfrom()  UDP
s.sendto()
s.getpeername()  连接到当前套接字的远端的地址
s.getsocketname()
s.getsocketopt()
s.setsocketopt()
s.close()

Blocking-Oriented Socket Methods
s.setblocking()    设置阻塞与非阻塞模式
s.settimeout()     设置阻塞套接字操作的超时时间
s.gettimeout()     得到阻塞套接字操作的超时时间

面向文件的套接字的函数
s.fileno()         套接字的文件描述符
s.makefile()       创建一个与该套接字关联的文件

### 16.4
SocketServer：   TCP

Server：

	#!/usr/bin/env python
	from  SocketServer import (TCPServer as TCP, StreamRequestHandler as SRH) 
	from  time import ctime
	HOST = ''
	PORT = 21567
	ADDR = (HOST, PORT)
	class  MyRequestHandler(SRH):
	    def  handle (self):
	        print '...connected form:', self.client_address
	        self.wfile.write('[%s] %s' %(ctime(), self.rfile.readline()))      
	tcpServ = TCP(ADDR, MyRequestHandler)
	print 'waiting for connection...'
	tcpServ.serve_forever()

Client：

	#!/usr/bin/env/python
	from socket import *
	HOST = 'localhost'
	PORT = 21567
	BUFSIZE = 1024
	ADDR = (HOST, PORT)
	while True:
	    tcpCliSock = socket(AF_INET, SOCK_STREAM)
	    tcpCliSock.connect(ADDR)
	    data = raw_input('>')
	    if not data:
	        break 
	    tcpCliSock.send('%s\r\n' % data)
	    data = tcpCliSock.recv(BUFSIZE)
	    if not data:
	        break 
	    print data.strip()
	    tcpCliSock.close()

### 16.5
Twisted 一个完全事件驱动的网络框架


## 17 网络客户端编程

FTP:
底层TCP; 客户端和服务器都使用2个套接字来通信;控制和命令端口(21号)+数据端口(有时20号)
FTP有两种模式：
主动：只有主动模式服务器才使用数据端口，设置20为数据端口，主动连接客户端数据端口
被动：服务器只是告诉客户端它的随机端口号码，客户端必须主动建立数据连接
导入ftplib模块，并实例化一个ftplib.FTP 类对象。
	from ftplib import FTP
	f = FTP('ftp.python.org')
	f.log('anonymous', 'guess@who.org')
	..
	f.quit()

ftplib.FTP 类方法

python2.0及之前版本被动模式支持是默认关闭的。
FTP客户端类型：1命令行客户端程序 2GUI客户端程序 3网页浏览器 4定制程序

### 17.3 网络新闻

Usenet新闻系统

NNTP--网络新闻传输协议：
只使用一个标准端口119 来做通讯

	from nntplib import NNTP
	n = NNTP('your.nntp.server')
	r,c,f,l,g = n.group('comp.long.python')
	..
	n.quit()

### 17.4 电子邮件

SMTP--简单邮件传输协议
POP3--邮局协议第三版
IMAP--交互式邮件访问协议

## 18 多线程编程

1 全局解释器锁 GIL，这个锁保证同一时刻只有一个线程运行在python虚拟机中

I/O密集型的python程序比计算密集型的程序更能充分利用多线程环境的好处
多线程编程模块：
threading(推荐)，
thread  不支持守护进程
Queue 供多线程使用的同步先进先出队列

## 20 web编程

HTTP(超文本传输协议) 是TCP/IP协议的上层协议， 通过发送接收HTTP消息来处理客户端的请求。
URL 统一资源定位器
python支持两种不同的模块,处理URL字符串: urlparse和urllib

### 20.2.3  核心模块：urllib
提供一个高级的web交流库，支持Web协议，HTTP,FTP, Gopher协议，支持本地文件的访问。
urlopen( )
urlretrieve( )

### 20.3 高级web客户端-一个爬虫
    #unfinished
	#!/usr/bin/env python
	from sys import argv
	from os import makedirs, unlink, sep
	from os.path import dirname, exists, isdir, splitext
	from string import replace, find, lower
	from htmllib import HTMLParser
	from urllib import urlretrieve
	from urlparse import urlparse, urljoin
	from formatter import DumbWriter, AbstractFormatter 
	from cStringIO import StringIO 

	class  Retriever(object):
	    """downLoad web pages"""
	    def __init__(self, url):
	        self.url = url
	        self.file = self.filename(url)

	    def filename(self, url, deffile='index.htm'):
	        parsedurl = urlparse(url, 'http:', 0)        #parse path
	        path = parsedurl[1] + parsedurl[2]
	        ext = splitext(path)
	        if ext[1]  == '':                               # no file, use default
	            if path[-1] == '/':
	                path += deffile
	            else:
	                path += '/' + deffile
	        ldir = dirname(path)                  #local directory
	        if sep != '/':       #os-indep. path separator
	            Idir = replace(ldir, '/', sep)
	            if not isdir(ldir):    #create archive dir if nec
	                if exists(ldir): unlink(ldir)
	            makedirs(ldir)
	            return path
	                    
	    def download(self):                                 #download Web page
	        try:
	            retval = urlretrieve(self.url, self.file)
	        except IOError:
	            retval = ('*** ERROR: invalid URL "%s"' %self.url) 
	        return retval

	    def parseAndGetLinks(self):                         #parse HTML, save links
	        self.parser = HTMLParser(AbstractFormatter(DumbWriter(StringIO())))
	        self.parser.feed(open(self.file).read())
	        self.parser.close()
	        return self.parser.anchorlist 
	    
	    
	    
	class Crawler(object):                                  #manage entire crawling process
	    
	    count = 0                                            #static downloaded page counter
	    
	    def  __init__ (self, url):
	        self.q = [url]
	        self.seen = []
	        self.dom = urlparse(url)[1]                     #urlparse模块主要是把url拆分为6部分，并返回元组
	        
	    def  getPage(self, url):
	        r = Retriever(url)
	        retval = r.download()
	        if retval[0] == '*':                             #error situation, do not parse
	            print retval, '... skipping parse'
	            return 
	        Crawler.count += 1
	        print '\n(', Crawler.count, ')'
	        print 'URL:', url
	        print 'FILE:', retval[0]
	        self.seen.append(url)
	        
	        links = r.parseAndGetLinks()                      #get and process links
	        for eachLink in links:
	            if eachLink[:4] != 'http' and find(eachLink, '://') == -1:
	                
	                eachLink = urljoin(url, eachLink)
	                print '*', eachLink
	                
	                if find(lower(eachLink), 'mailto:') != -1:
	                    print '... discarded, mailto link'
	                    continue
	                
	                if eachLink not in self.seen:
	                    if find(eachLink, self.dom) == -1:
	                        print '... discarded, not in domain'
	                    else:
	                        if eachLink not in self.q:
	                            self.q.append(eachLink)
	                            print '... new, added to Q'
	                        else:
	                            print '... discarded, already in Q'
	                else:
	                    print '... discarded, already processed'
	                    
	    def  go(self):   #process links inqueue
	        while self.q:
	            url = self.q.pop()
	            self.getPage(url)
	        
	def  main():
	    if len(argv) > 1:
	        url = argv[1]
	    else:
	        try:
	            #url = raw_input('Enter starting URL: ')
	            url = "http://www.cnbeta.com"
	        except (KeyboardInterrupt, EOFError):
	            url = ''
	       
	    if not url: return
	    robot = Crawler(url)
	    robot.go()
	            
	if __name__ == '__main__':
	    main()